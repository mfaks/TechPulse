services:

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    environment:
      - GITHUB_CLIENT_ID=${GITHUB_CLIENT_ID}
      - GITHUB_CLIENT_SECRET=${GITHUB_CLIENT_SECRET}
      - GOOGLE_CLIENT_ID=${GOOGLE_CLIENT_ID}
      - GOOGLE_CLIENT_SECRET=${GOOGLE_CLIENT_SECRET}
      - SESSION_SECRET=${SESSION_SECRET}
      - FRONTEND_URL=${FRONTEND_URL}
      - BACKEND_URL=${BACKEND_URL}
      - LOCALSTACK=${LOCALSTACK}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_REGION=${AWS_REGION}
      - KAFKA_SERVER=${KAFKA_SERVER}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    volumes:
      - ./backend:/app
    depends_on:
      - localstack

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "5173:5173"
    environment:
      - VITE_BACKEND_URL=${VITE_BACKEND_URL}
    volumes:
      - ./frontend:/app
      - /app/node_modules

  localstack:
    image: localstack/localstack:latest
    ports:
      - "4566:4566"
    environment:
      - SERVICES=dynamodb,lambda
      - DEBUG=1
      - PERSISTENCE=0
      - AWS_REGION=${AWS_REGION}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - LAMBDA_EXECUTOR=local
      - DOCKER_HOST=unix:///var/run/docker.sock
      - DISABLE_CORS_CHECKS=1
      - INIT_SCRIPTS_PATH=/docker-entrypoint-initaws.d
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock"
      - ./backend/src/aws:/docker-entrypoint-initaws.d

  postgres:
    image: postgres:latest
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5433:5432"
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "${POSTGRES_USER}"]
      interval: 5s
      retries: 5

  airflow:
    image: apache/airflow:latest
    environment:
      - AIRFLOW__CORE__EXECUTOR=${AIRFLOW__CORE__EXECUTOR}
      - AIRFLOW__CORE__LOAD_EXAMPLES=${AIRFLOW__CORE__LOAD_EXAMPLES}
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=${AIRFLOW__CORE__SQL_ALCHEMY_CONN}
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW__CORE__FERNET_KEY}
      - AIRFLOW__CORE__DAGS_FOLDER=${AIRFLOW__CORE__DAGS_FOLDER}
    volumes:
      - ./backend/src/airflow/dags:/opt/airflow/dags
      - ./backend/src/scrapers:/opt/airflow/scrapers
      - airflow_logs:/opt/airflow/logs
    ports:
      - "8080:8080"
    command: >
      bash -c "
      pip install apache-airflow-providers-amazon requests beautifulsoup4 feedparser boto3 &&
      echo 'Waiting for PostgreSQL to be ready...' &&
      while ! nc -z postgres 5432; do sleep 1; done &&
      echo 'PostgreSQL is ready!' &&
      airflow db init &&
      airflow connections delete aws_default || true &&
      airflow connections add aws_default --conn-type aws --conn-login ${AWS_ACCESS_KEY_ID} --conn-password ${AWS_SECRET_ACCESS_KEY} --conn-extra '{\"region_name\": \"${AWS_REGION}\", \"endpoint_url\": \"${LOCALSTACK}\"}' &&
      airflow users create --username ${AIRFLOW_USER} --password ${AIRFLOW_PASSWORD} --firstname ${AIRFLOW_FIRSTNAME} --lastname ${AIRFLOW_LASTNAME} --role Admin --email ${AIRFLOW_EMAIL} &&
      airflow webserver & 
      sleep 30 &&
      airflow dags unpause web_scraping_dag &&
      sleep 5 &&
      airflow dags trigger web_scraping_dag &&
      wait"
    depends_on:
      postgres:
        condition: service_healthy


  airflow-scheduler:
    image: apache/airflow:latest
    environment:
      - AIRFLOW__CORE__EXECUTOR=${AIRFLOW__CORE__EXECUTOR}
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=${AIRFLOW__CORE__SQL_ALCHEMY_CONN}
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW__CORE__FERNET_KEY}
      - AIRFLOW__CORE__DAGS_FOLDER=${AIRFLOW__CORE__DAGS_FOLDER}
    volumes:
      - ./backend/src/airflow/dags:/opt/airflow/dags
      - ./backend/src/scrapers:/opt/airflow/scrapers
      - airflow_logs:/opt/airflow/logs
    command: >
      bash -c "
      pip install apache-airflow-providers-amazon requests beautifulsoup4 feedparser boto3 lxml &&
      echo 'Waiting for Airflow webserver to be ready...' &&
      while ! nc -z airflow 8080; do sleep 1; done &&
      echo 'Airflow webserver is ready!' &&
      airflow scheduler
      "
    depends_on:
      - airflow
      - postgres

  kafka:
    image: bitnami/kafka:latest
    environment:
      - KAFKA_BROKER_ID=${KAFKA_BROKER_ID}
      - KAFKA_CFG_ZOOKEEPER_CONNECT=${KAFKA_CFG_ZOOKEEPER_CONNECT}
      - ALLOW_PLAINTEXT_LISTENER=${ALLOW_PLAINTEXT_LISTENER}
      - KAFKA_CFG_LISTENERS=${KAFKA_CFG_LISTENERS}
      - KAFKA_CFG_ADVERTISED_LISTENERS=${KAFKA_CFG_ADVERTISED_LISTENERS}
      - KAFKA_SERVER=${KAFKA_SERVER}
    ports:
      - "9092:9092"
    depends_on:
      - zookeeper

  zookeeper:
    image: bitnami/zookeeper:latest
    environment:
      - ALLOW_ANONYMOUS_LOGIN=${ALLOW_ANONYMOUS_LOGIN}
    ports:
      - "2181:2181"

  query-service:
    build:
      context: ./backend
      dockerfile: Dockerfile.query
    environment:
      - KAFKA_SERVER=${KAFKA_SERVER}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_REGION=${AWS_REGION}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - LOCALSTACK=${LOCALSTACK}
    volumes:
      - ./backend:/app
    depends_on:
      - kafka
      - localstack
      - openai-service

  openai-service:
    build:
      context: ./backend
      dockerfile: Dockerfile.openai
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - KAFKA_SERVER=${KAFKA_SERVER}
    volumes:
      - ./backend:/app
    depends_on:
      - kafka

volumes:
  postgres_data:
  airflow_logs: